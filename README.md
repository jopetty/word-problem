# Word Problems

Data and code to evaluate how deep transformer models need to be to learn group multiplication.

## Installation

Dependencies are specified in `pyproject.toml` and can be installed with any Python package manger (e.g., `pip` or `poetry`). For convenience, we also provide a Conda `environment.yml` file which wraps around `pyproject.toml`. To create the environment and install all dependencies, run

```bash
conda env create && conda activate depth
```

## Use

### Training Transformer Models

To train a model, run `python src/main.py train`. Command-line arguments to configure the training run are the arguments to the `train` function, which is the most accurate documentation for what to do. Some important ones:

- `--group`: the name of the group to train on.
- `--k`: the length of the sequences to train on. Must be greater than 1.
- `--num_layers`: how many layers in the transformer encoder.
- `--epochs`: the number of epochs to train for.

Training on sequences of length `k` means that all `group=k` sequences will be split between the train and test sets. Additionally, all sequences of length 2 <= `m` < `k` will be included in the training set; to _only_ train on sequences of length 2 and `k`, pass the `--strict_len` flag.

```bash
# Trains a model on all sequences of length 2, 3, 4 on data from S5
python src/main.py train --group S5 --k 4

# Trains a model only on sequences of length 2 and 4 on data from S5
python src/main.py train --group S5 --k 4 --strict_len
```

The combination of `group` and `k` determines which data files to use. Data files are stored by default in the `data/` directory and have the name `group=k.csv`.

### Training MLP Baselines

As a sanity check for what a single-layer should be able to compute, we can train a MLP to learn binary multiplication. We don't train with a train/test split since we are only concerned with whether or not a single layer can learn the function, not how well it generalizes. The only required argument is `--group`.

```bash
# Train a single-layer MLP on binary multiplication in Z60
python src/main.py train_mlp --group Z60
```

### Generating Data

Data files are generated by calling `python src/generate_data.py` with the following arguments:

- `--group`: A string representing the group to use. Supported groups are of the form `G#`, where `G` is one of `S` (symmetric), `A` (alternating), or `Z` (cyclic), and `#` is an integer. You can also use the direct products of any of these groups by separating each with a `_x_`. So `--group=S5` generates data using elements of `S5` and `--group=A5_x_Z9` generates data using elements of `A5 x Z9`.
- `--k`: The length of the sequences to generate. Each sequence consists of `k` elements multiplied together.
- `--samples`: The number of examples to generate. If this is left blank, it will generate the maximum number of distinct sequences possible for the given group and sequence length, equal to `#(G)^k`. If this is set to be an integer, it will generate `min(samples, #(G)^k)` examples; note that we cap the number of examples to ensure that any partitions of the generate datasets are guaranteed to be sequence-wise disjoint. If `samples` is less than `#(G)^k`, examples will be generated randomly without replacement.
- `--data_dir`: The directory to save the data to. If this is left blank, it will save to a `data/` directory in the project root.
- `--seed`: The random seed to use for generating the data.
- `--overwrite`: Whether to overwrite an existing data file for the given values for `group` and `k`.

```bash
# Generate 100k 5-element sequences & their reductions from S5
python src/generate_data.py --group S5 --k 5 --samples 100000
```

Data files are named `group=k.csv`.

```csv
length,input,target
```

where `length` is equal to `k`, `input` is a series of space-separated integers corresponding to the element index of the group as defined in the `abstract_algebra` object, and `target` is the element the sequence of input elements multiplies to (again, as the element index of the group object).

By default, we include data files for `Z60`, `A4_x_Z5`, and `A5` for sequence lengths 2 through 10, subsampled to a maximum of 5,000,000 examples per `k`.

### Logging

Run data is logged to Weights&Biases; add `WANDB_API_KEY=##################` to a `.env` file in the root directory of the project to automatically configure the W&B logger. If you don't want to log runs to W&B, pass the `--nologging` flag.
